# coding: utf-8

"""
    SciCat backend API

    This is the API for the SciCat Backend ============================================

    The version of the OpenAPI document: 4.1.1
    Generated by OpenAPI Generator (https://openapi-generator.tech)

    Do not edit the class manually.
"""  # noqa: E501


from __future__ import annotations
import pprint
import re  # noqa: F401
import json

from datetime import datetime
from typing import Any, Dict, List, Optional, Union
from pydantic import BaseModel, Field, StrictBool, StrictFloat, StrictInt, StrictStr, conlist, validator
from Scicat-pydantic-SDK-test.models.attachment import Attachment
from Scicat-pydantic-SDK-test.models.datablock import Datablock
from Scicat-pydantic-SDK-test.models.history_class import HistoryClass
from Scicat-pydantic-SDK-test.models.lifecycle_class import LifecycleClass
from Scicat-pydantic-SDK-test.models.orig_datablock import OrigDatablock
from Scicat-pydantic-SDK-test.models.relationship_class import RelationshipClass
from Scicat-pydantic-SDK-test.models.technique_class import TechniqueClass

class DatasetClass(BaseModel):
    """
    DatasetClass
    """
    created_by: StrictStr = Field(default=..., alias="createdBy", description="Indicate the user who created this record. This property is added and maintained by the system.")
    updated_by: StrictStr = Field(default=..., alias="updatedBy", description="Indicate the user who updated this record last. This property is added and maintained by the system.")
    created_at: datetime = Field(default=..., alias="createdAt", description="Date and time when this record was created. This field is managed by mongoose with through the timestamp settings. The field should be a string containing a date in ISO 8601 format (2024-02-27T12:26:57.313Z)")
    updated_at: datetime = Field(default=..., alias="updatedAt", description="Date and time when this record was updated last. This field is managed by mongoose with through the timestamp settings. The field should be a string containing a date in ISO 8601 format (2024-02-27T12:26:57.313Z)")
    owner_group: StrictStr = Field(default=..., alias="ownerGroup", description="Defines the group which owns the data, and therefore has unrestricted access to this data. Usually a pgroup like p12151")
    access_groups: conlist(StrictStr) = Field(default=..., alias="accessGroups", description="Optional additional groups which have read access to the data. Users which are members in one of the groups listed here are allowed to access this data. The special group 'public' makes data available to all users.")
    instrument_group: Optional[StrictStr] = Field(default=None, alias="instrumentGroup", description="Optional additional groups which have read and write access to the data. Users which are members in one of the groups listed here are allowed to access this data.")
    is_published: StrictBool = Field(default=..., alias="isPublished", description="Flag is true when data are made publicly available.")
    pid: StrictStr = Field(default=..., description="Persistent Identifier for datasets derived from UUIDv4 and prepended automatically by site specific PID prefix like 20.500.12345/")
    owner: StrictStr = Field(default=..., description="Owner or custodian of the dataset, usually first name + last name. The string may contain a list of persons, which should then be separated by semicolons.")
    owner_email: Optional[StrictStr] = Field(default=None, alias="ownerEmail", description="Email of the owner or custodian of the dataset. The string may contain a list of emails, which should then be separated by semicolons.")
    orcid_of_owner: Optional[StrictStr] = Field(default=None, alias="orcidOfOwner", description="ORCID of the owner or custodian. The string may contain a list of ORCIDs, which should then be separated by semicolons.")
    contact_email: StrictStr = Field(default=..., alias="contactEmail", description="Email of the contact person for this dataset. The string may contain a list of emails, which should then be separated by semicolons.")
    source_folder: StrictStr = Field(default=..., alias="sourceFolder", description="Absolute file path on file server containing the files of this dataset, e.g. /some/path/to/sourcefolder. In case of a single file dataset, e.g. HDF5 data, it contains the path up to, but excluding the filename. Trailing slashes are removed.")
    source_folder_host: Optional[StrictStr] = Field(default=None, alias="sourceFolderHost", description="DNS host name of file server hosting sourceFolder, optionally including a protocol e.g. [protocol://]fileserver1.example.com")
    size: Union[StrictFloat, StrictInt] = Field(default=..., description="Total size of all source files contained in source folder on disk when unpacked.")
    packed_size: Optional[Union[StrictFloat, StrictInt]] = Field(default=0, alias="packedSize", description="Total size of all datablock package files created for this dataset.")
    number_of_files: Union[StrictFloat, StrictInt] = Field(default=..., alias="numberOfFiles", description="Total number of files in all OrigDatablocks for this dataset.")
    number_of_files_archived: Optional[Union[StrictFloat, StrictInt]] = Field(default=0, alias="numberOfFilesArchived", description="Total number of files in all Datablocks for this dataset.")
    creation_time: datetime = Field(default=..., alias="creationTime", description="Time when dataset became fully available on disk, i.e. all containing files have been written. Format according to chapter 5.6 internet date/time format in RFC 3339. Local times without timezone/offset info are automatically transformed to UTC using the timezone of the API server.")
    type: StrictStr = Field(default=..., description="Characterize type of dataset, either 'raw' or 'derived'. Autofilled when choosing the proper inherited models.")
    validation_status: Optional[StrictStr] = Field(default=None, alias="validationStatus", description="Defines a level of trust, e.g. a measure of how much data was verified or used by other persons.")
    keywords: Optional[conlist(StrictStr)] = Field(default=None, description="Array of tags associated with the meaning or contents of this dataset. Values should ideally come from defined vocabularies, taxonomies, ontologies or knowledge graphs.")
    description: Optional[StrictStr] = Field(default=None, description="Free text explanation of contents of dataset.")
    dataset_name: Optional[StrictStr] = Field(default=None, alias="datasetName", description="A name for the dataset, given by the creator to carry some semantic meaning. Useful for display purposes e.g. instead of displaying the pid. Will be autofilled if missing using info from sourceFolder.")
    classification: Optional[StrictStr] = Field(default=None, description="ACIA information about AUthenticity,COnfidentiality,INtegrity and AVailability requirements of dataset. E.g. AV(ailabilty)=medium could trigger the creation of a two tape copies. Format 'AV=medium,CO=low'")
    license: Optional[StrictStr] = Field(default=None, description="Name of the license under which the data can be used.")
    version: Optional[StrictStr] = Field(default=None, description="Version of the API used in creation of the dataset.")
    history: Optional[conlist(HistoryClass)] = Field(default=None, description="List of objects containing old and new values.")
    datasetlifecycle: Optional[LifecycleClass] = Field(default=None, description="Describes the current status of the dataset during its lifetime with respect to the storage handling systems.")
    techniques: Optional[conlist(TechniqueClass)] = Field(default=None, description="Stores the metadata information for techniques.")
    relationships: Optional[conlist(RelationshipClass)] = Field(default=None, description="Stores the relationships with other datasets.")
    shared_with: Optional[conlist(StrictStr)] = Field(default=None, alias="sharedWith", description="List of users that the dataset has been shared with.")
    attachments: Optional[conlist(Attachment)] = Field(default=None, description="Small, less than 16 MB attachments, envisaged for png/jpeg previews.")
    origdatablocks: Optional[conlist(OrigDatablock)] = Field(default=None, description="Containers that list all files and their attributes which make up a dataset. Usually filled at the time the dataset's metadata is created in the data catalog. Can be used by subsequent archiving processes to create the archived datasets.")
    datablocks: Optional[conlist(Datablock)] = Field(default=None, description="When archiving a dataset, all files contained in the dataset are listed here together with their checksum information. Several datablocks can be created if the file listing is too long for a single datablock. This partitioning decision is done by the archiving system to allow for chunks of datablocks with manageable sizes. E.g a datasets consisting of 10 TB of data could be split into 10 datablocks of about 1 TB each. The upper limit set by the data catalog system itself is given by the fact that documents must be smaller than 16 MB, which typically allows for datasets of about 100000 files.")
    scientific_metadata: Optional[Dict[str, Any]] = Field(default=None, alias="scientificMetadata", description="JSON object containing the scientific metadata.")
    comment: Optional[StrictStr] = Field(default=None, description="Comment the user has about a given dataset.")
    data_quality_metrics: Optional[Union[StrictFloat, StrictInt]] = Field(default=None, alias="dataQualityMetrics", description="Data Quality Metrics given by the user to rate the dataset.")
    principal_investigator: Optional[StrictStr] = Field(default=None, alias="principalInvestigator", description="First name and last name of principal investigator(s). If multiple PIs are present, use a semicolon separated list. This field is required if the dataset is a Raw dataset.")
    end_time: Optional[datetime] = Field(default=None, alias="endTime", description="End time of data acquisition for this dataset, format according to chapter 5.6 internet date/time format in RFC 3339. Local times without timezone/offset info are automatically transformed to UTC using the timezone of the API server.")
    creation_location: StrictStr = Field(default=..., alias="creationLocation", description="Unique location identifier where data was taken, usually in the form /Site-name/facility-name/instrumentOrBeamline-name. This field is required if the dataset is a Raw dataset.")
    data_format: Optional[StrictStr] = Field(default=None, alias="dataFormat", description="Defines the format of the data files in this dataset, e.g Nexus Version x.y.")
    proposal_id: Optional[StrictStr] = Field(default=None, alias="proposalId", description="The ID of the proposal to which the dataset belongs.")
    sample_id: Optional[StrictStr] = Field(default=None, alias="sampleId", description="ID of the sample used when collecting the data.")
    instrument_id: Optional[StrictStr] = Field(default=None, alias="instrumentId", description="ID of the instrument where the data was created.")
    investigator: Optional[StrictStr] = Field(default=None, description="First name and last name of the person or people pursuing the data analysis. The string may contain a list of names, which should then be separated by semicolons.")
    input_datasets: Optional[conlist(StrictStr)] = Field(default=None, alias="inputDatasets", description="Array of input dataset identifiers used in producing the derived dataset. Ideally these are the global identifier to existing datasets inside this or federated data catalogs. This field is required if the dataset is a Derived dataset.")
    used_software: Optional[conlist(StrictStr)] = Field(default=None, alias="usedSoftware", description="A list of links to software repositories which uniquely identifies the pieces of software, including versions, used for yielding the derived data. This field is required if the dataset is a Derived dataset.")
    job_parameters: Optional[Dict[str, Any]] = Field(default=None, alias="jobParameters", description="The creation process of the derived data will usually depend on input job parameters. The full structure of these input parameters are stored here.")
    job_log_data: Optional[StrictStr] = Field(default=None, alias="jobLogData", description="The output job logfile. Keep the size of this log data well below 15 MB.")
    __properties = ["createdBy", "updatedBy", "createdAt", "updatedAt", "ownerGroup", "accessGroups", "instrumentGroup", "isPublished", "pid", "owner", "ownerEmail", "orcidOfOwner", "contactEmail", "sourceFolder", "sourceFolderHost", "size", "packedSize", "numberOfFiles", "numberOfFilesArchived", "creationTime", "type", "validationStatus", "keywords", "description", "datasetName", "classification", "license", "version", "history", "datasetlifecycle", "techniques", "relationships", "sharedWith", "attachments", "origdatablocks", "datablocks", "scientificMetadata", "comment", "dataQualityMetrics", "principalInvestigator", "endTime", "creationLocation", "dataFormat", "proposalId", "sampleId", "instrumentId", "investigator", "inputDatasets", "usedSoftware", "jobParameters", "jobLogData"]

    @validator('type')
    def type_validate_enum(cls, value):
        """Validates the enum"""
        if value not in ('raw', 'derived'):
            raise ValueError("must be one of enum values ('raw', 'derived')")
        return value

    class Config:
        """Pydantic configuration"""
        allow_population_by_field_name = True
        validate_assignment = True

    def to_str(self) -> str:
        """Returns the string representation of the model using alias"""
        return pprint.pformat(self.dict(by_alias=True))

    def to_json(self) -> str:
        """Returns the JSON representation of the model using alias"""
        return json.dumps(self.to_dict())

    @classmethod
    def from_json(cls, json_str: str) -> DatasetClass:
        """Create an instance of DatasetClass from a JSON string"""
        return cls.from_dict(json.loads(json_str))

    def to_dict(self):
        """Returns the dictionary representation of the model using alias"""
        _dict = self.dict(by_alias=True,
                          exclude={
                          },
                          exclude_none=True)
        # override the default output from pydantic by calling `to_dict()` of each item in history (list)
        _items = []
        if self.history:
            for _item in self.history:
                if _item:
                    _items.append(_item.to_dict())
            _dict['history'] = _items
        # override the default output from pydantic by calling `to_dict()` of datasetlifecycle
        if self.datasetlifecycle:
            _dict['datasetlifecycle'] = self.datasetlifecycle.to_dict()
        # override the default output from pydantic by calling `to_dict()` of each item in techniques (list)
        _items = []
        if self.techniques:
            for _item in self.techniques:
                if _item:
                    _items.append(_item.to_dict())
            _dict['techniques'] = _items
        # override the default output from pydantic by calling `to_dict()` of each item in relationships (list)
        _items = []
        if self.relationships:
            for _item in self.relationships:
                if _item:
                    _items.append(_item.to_dict())
            _dict['relationships'] = _items
        # override the default output from pydantic by calling `to_dict()` of each item in attachments (list)
        _items = []
        if self.attachments:
            for _item in self.attachments:
                if _item:
                    _items.append(_item.to_dict())
            _dict['attachments'] = _items
        # override the default output from pydantic by calling `to_dict()` of each item in origdatablocks (list)
        _items = []
        if self.origdatablocks:
            for _item in self.origdatablocks:
                if _item:
                    _items.append(_item.to_dict())
            _dict['origdatablocks'] = _items
        # override the default output from pydantic by calling `to_dict()` of each item in datablocks (list)
        _items = []
        if self.datablocks:
            for _item in self.datablocks:
                if _item:
                    _items.append(_item.to_dict())
            _dict['datablocks'] = _items
        return _dict

    @classmethod
    def from_dict(cls, obj: dict) -> DatasetClass:
        """Create an instance of DatasetClass from a dict"""
        if obj is None:
            return None

        if not isinstance(obj, dict):
            return DatasetClass.parse_obj(obj)

        _obj = DatasetClass.parse_obj({
            "created_by": obj.get("createdBy"),
            "updated_by": obj.get("updatedBy"),
            "created_at": obj.get("createdAt"),
            "updated_at": obj.get("updatedAt"),
            "owner_group": obj.get("ownerGroup"),
            "access_groups": obj.get("accessGroups"),
            "instrument_group": obj.get("instrumentGroup"),
            "is_published": obj.get("isPublished") if obj.get("isPublished") is not None else False,
            "pid": obj.get("pid"),
            "owner": obj.get("owner"),
            "owner_email": obj.get("ownerEmail"),
            "orcid_of_owner": obj.get("orcidOfOwner"),
            "contact_email": obj.get("contactEmail"),
            "source_folder": obj.get("sourceFolder"),
            "source_folder_host": obj.get("sourceFolderHost"),
            "size": obj.get("size") if obj.get("size") is not None else 0,
            "packed_size": obj.get("packedSize") if obj.get("packedSize") is not None else 0,
            "number_of_files": obj.get("numberOfFiles") if obj.get("numberOfFiles") is not None else 0,
            "number_of_files_archived": obj.get("numberOfFilesArchived") if obj.get("numberOfFilesArchived") is not None else 0,
            "creation_time": obj.get("creationTime"),
            "type": obj.get("type"),
            "validation_status": obj.get("validationStatus"),
            "keywords": obj.get("keywords"),
            "description": obj.get("description"),
            "dataset_name": obj.get("datasetName"),
            "classification": obj.get("classification"),
            "license": obj.get("license"),
            "version": obj.get("version"),
            "history": [HistoryClass.from_dict(_item) for _item in obj.get("history")] if obj.get("history") is not None else None,
            "datasetlifecycle": LifecycleClass.from_dict(obj.get("datasetlifecycle")) if obj.get("datasetlifecycle") is not None else None,
            "techniques": [TechniqueClass.from_dict(_item) for _item in obj.get("techniques")] if obj.get("techniques") is not None else None,
            "relationships": [RelationshipClass.from_dict(_item) for _item in obj.get("relationships")] if obj.get("relationships") is not None else None,
            "shared_with": obj.get("sharedWith"),
            "attachments": [Attachment.from_dict(_item) for _item in obj.get("attachments")] if obj.get("attachments") is not None else None,
            "origdatablocks": [OrigDatablock.from_dict(_item) for _item in obj.get("origdatablocks")] if obj.get("origdatablocks") is not None else None,
            "datablocks": [Datablock.from_dict(_item) for _item in obj.get("datablocks")] if obj.get("datablocks") is not None else None,
            "scientific_metadata": obj.get("scientificMetadata"),
            "comment": obj.get("comment"),
            "data_quality_metrics": obj.get("dataQualityMetrics"),
            "principal_investigator": obj.get("principalInvestigator"),
            "end_time": obj.get("endTime"),
            "creation_location": obj.get("creationLocation"),
            "data_format": obj.get("dataFormat"),
            "proposal_id": obj.get("proposalId"),
            "sample_id": obj.get("sampleId"),
            "instrument_id": obj.get("instrumentId"),
            "investigator": obj.get("investigator"),
            "input_datasets": obj.get("inputDatasets"),
            "used_software": obj.get("usedSoftware"),
            "job_parameters": obj.get("jobParameters"),
            "job_log_data": obj.get("jobLogData")
        })
        return _obj


