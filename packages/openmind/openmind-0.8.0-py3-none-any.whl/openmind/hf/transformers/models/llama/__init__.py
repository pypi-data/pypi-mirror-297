from .modeling_llama import LlamaNpuFlashAttention, _update_llama_model_causal_mask
