Metadata-Version: 2.1
Name: jaims-py
Version: 2.0.0
Summary: A Python package for creating LLM powered, agentic, platform agnostic software.
Home-page: https://github.com/dev-mush/jaims-py
Author: Marco Musella
License: MIT
Keywords: An extensible library to create LLM Agents and LLM based applications.
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: Pillow>=10.3.0
Requires-Dist: pydantic>=2.0.0
Requires-Dist: jsonref>=1.1.0
Provides-Extra: all
Requires-Dist: openai>=1.34.0; extra == "all"
Requires-Dist: tiktoken>=0.7.0; extra == "all"
Requires-Dist: google-generativeai>=0.6.0; extra == "all"
Requires-Dist: google-cloud-aiplatform>=1.0.0; extra == "all"
Requires-Dist: mistralai==0.4.2; extra == "all"
Requires-Dist: anthropic>=0.31.1; extra == "all"
Requires-Dist: anthropic[vertex]>=0.31.1; extra == "all"
Provides-Extra: anthropic
Requires-Dist: anthropic>=0.31.1; extra == "anthropic"
Provides-Extra: anthropic-all
Requires-Dist: anthropic>=0.31.1; extra == "anthropic-all"
Requires-Dist: anthropic[vertex]>=0.31.1; extra == "anthropic-all"
Provides-Extra: anthropic-vertex
Requires-Dist: anthropic[vertex]>=0.31.1; extra == "anthropic-vertex"
Provides-Extra: google
Requires-Dist: google-generativeai>=0.6.0; extra == "google"
Provides-Extra: mistral
Requires-Dist: mistralai==0.4.2; extra == "mistral"
Provides-Extra: openai
Requires-Dist: openai>=1.34.0; extra == "openai"
Requires-Dist: tiktoken>=0.7.0; extra == "openai"
Provides-Extra: vertexai
Requires-Dist: google-cloud-aiplatform>=1.0.0; extra == "vertexai"

<p align="center">
    <img width="300" src="https://github.com/dev-mush/jaims-py/assets/669003/5c53381f-25b5-4141-bcd2-7457863eafb9" >
</p>

# JAIms

_My name is Bot, JAIms Bot._ üï∂Ô∏è

JAIms is a lightweight Python package that lets you build powerful LLM-Based agents or LLM powered applications with ease. It is platform agnostic, so you can focus on integrating AI into your software and let JAIms handle the boilerplate of communicating with the LLM API.
The main goal of JAIms is to provide a simple and easy-to-use interface to leverage the power of LLMs in your software, without having to worry about the specifics of the underlying provider, and to seamlessly integrate LLM functionality with your own codebase.
JAIms currently supports mainstream foundation LLMs such as OpenAI's GPT models, Google's gemini models (also on Vertex), Mistral models and Anthropic Models (both hosted on Anthropic and Vertex endpoints). JAIms can be easily extended to connect to your own model and endpoints.

Check out the [getting started guide](docs/getting_started.md) to quickly get up and running with JAIms.

Also consider checking out the [examples](examples) folder for more advanced use cases.

### ‚ú® Main Features

- Built in support for most common foundational models.
- Built in conversation history management to allow fast creation of chatbots, this can be easily extended to support more advanced history management strategies.
- Image support for multimodal LLMs üñºÔ∏è.
- Support for function calling, both streamed and non-streamed.
- Fast integration with dataclasses and pydantic models.
- Error handling and exponential backoff for built in providers (openai, google, mistral)

### üß† Guiding Principles

JAIms comes out of the necessity for a lightweight and easy-to-use framework to create LLM agents or integrate LLM functionality in python projects. Given the increasing work with both foundational and open source LLMs, JAIms has been designed as an abstraction layer to streamline fast creation of agentic business logic and seamless codebase integration.

In case you like to contribute, please keep in mind that I try to keep the code:

- **Modular**: any component is provided with a default basic implementation and an interface that can be easily extended for more complex use cases.
- **Type Hinted and Explicit**: I've done my best to type hint everything and document the codebase as much as possible to avoid digging into the code.
- **Tested**: Well...Let's just say I could have done better, but am planning to improve code coverage and test automation in the near future.
- **Application focused**: I'm not trying to build a library similar to langchain or llamaindex to perform data-driven operations on LLMs, I'm trying to build a very simple and lightweight framework that leverages the possibility of LLMs to perform function calling so that LLMs can easily be integrated in software applications.
- **Extensible**: I'm planning to add more providers and more features.

## ‚ö†Ô∏è Project status

I'm using this library in many of my projects without problems, that said I've just revamped it entirely to support multiple providers and entirely refactored the codebase to streamline function calling. I've done my best to test it thoroughly, but I can't guarantee something won't break.

In the [roadmap](docs/roadmap.md) I'm tracking the next steps I'm planning to take to improve the library.

I'm actively working on this project and I'm open to contributions, so feel free to open an issue or a PR if you find something that needs fixing or improving.

Since I've started the development of JAIms, a few similar projects have been started, and granted that I didn't have time to check them out yet, some might easily be more advanced, yet I've widely employed this library in my projects and those of the company I work for, and I've been actively maintaining it, so I'm planning to keep it up to date and to improve it as much as I can.

I've opted for an open source by default approach to allow others to benefit from it and force myself to keep the code clean and well documented, just remember that since this is, for now, a side-project developed just by me (that am fairly new to python), expect the possibility of encountering some issues and don't expect an immediate patch from me, any help is very much appreciated ü§ó.

## üìù License

Copyright (c) 2023 Marco Musella (aka Mush). This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.
