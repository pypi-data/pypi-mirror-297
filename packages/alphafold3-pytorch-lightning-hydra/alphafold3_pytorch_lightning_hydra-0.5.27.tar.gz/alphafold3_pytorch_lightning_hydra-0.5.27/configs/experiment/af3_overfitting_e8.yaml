# @package _global_

# lists the experiment parameters corresponding to an "Overfitting Experiment" with 8 training examples

# to execute this experiment run:
# python train.py experiment=af3_overfitting_e8

defaults:
  - override /callbacks: default
  - override /data: pdb
  - override /environment: default
  - override /logger: wandb
  - override /model: alphafold3
  - override /strategy: deepspeed
  - override /trainer: deepspeed

# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters

tags:
  [
    "pdb",
    "alphafold3",
    "overfitting",
    "721p",
    "209d",
    "7a4d",
    "1a6a",
    "1g5a",
    "1js0",
    "1pu0",
    "4ulv",
  ]

seed: 12345

# overfitting experiment parameters:

data:
  crop_size: 384
  max_msas_per_chain: 16384
  max_num_msa_tokens: 41943040
  max_templates_per_chain: 20
  num_templates_per_chain: 4
  max_num_template_tokens: 5120
  max_train_length: 10240
  max_val_length: 2560

  overfitting_train_examples: true
  sample_only_pdb_ids:
    [
      721p-assembly1,
      209d-assembly1,
      7a4d-assembly1,
      1a6a-assembly1,
      "1g5a-assembly1",
      1js0-assembly1,
      1pu0-assembly1,
      4ulv-assembly1,
    ]

logger:
  wandb:
    entity: bml-lab
    group: "af3-overfitting-experiments"
    tags: ${tags}
    name: e8-${now:%Y%m%d%H%M%S}

model:
  diffusion_add_smooth_lddt_loss: true
  diffusion_add_bond_loss: true
  visualize_val_samples_every_n_steps: 1

trainer:
  min_steps: null
  max_steps: -1
  min_epochs: 1 # NOTE: prevents early stopping
  max_epochs: 20000
  check_val_every_n_epoch: null
  val_check_interval: 50
  log_every_n_steps: 1
# NOTE: the following argument is only needed when using the `fsdp` strategy
# strategy:
#   ignored_modules: [network.plms]
