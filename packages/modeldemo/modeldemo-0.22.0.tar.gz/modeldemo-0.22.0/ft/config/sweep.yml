# W&B config (overrides corresponding values in config.yml)
project: modeldemo
method: random
metric:
  name: val_loss
  goal: minimize
parameters:
  ## Data Load
  total_bs:
    values: [4096, 8192, 16384, 32768, 65536]  # 2**12 (4bs * 1024seq_len * 1gpu) to 2**16 (< 100k/100M)
  bs:
    values: [1, 2, 4]
  seq_len:
    values: [128, 256, 512, 1024]
  ## Lora
  rank:
    values: [16, 32, 64, 128, 256]
  alpha:
    values: [16, 32, 64, 128, 256]
  dropout:
    min: 0.0
    max: 0.3
  use_rslora:
    values: [True, False]
  init_lora_weights:
    values: ["gaussian", "olora", "pissa", "loftq"]
  ## Optimizer (AdamW)
  max_lr:
    distribution: log_uniform_values
    min: !!float 1e-8
    max: 1.0
  beta1:
    min: 0.5
    max: 0.9
  beta2:
    min: 0.9
    max: 0.999
  eps:
    distribution: log_uniform_values
    min: !!float 1e-9
    max: !!float 1e-7
  wd:
    distribution: log_uniform_values
    min: !!float 1e-8
    max: 1.0
  ## LR Scheduler (OneCycle)
  pct_start:
    min: 0.1
    max: 0.9
  anneal_strategy:
    values: [cos, linear]
  cycle_momentum:
    values: [True, False]
  base_momentum:
    min: 0.75
    max: 0.85
  max_momentum:
    min: 0.85
    max: 0.99
  div_factor:
    min: 10.0
    max: 50.0
  final_div_factor:
    distribution: log_uniform_values
    min: 100
    max: 100000
  three_phase:
    values: [True, False]
  ## Loop
  log_freq:
    value: 1
  max_steps:
    value: 16
  eval_interval:
    value: 16
  val_steps:
    value: 20
  log_ckpt:
    value: False
