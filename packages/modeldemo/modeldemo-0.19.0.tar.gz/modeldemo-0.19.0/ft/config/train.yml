# seed
seed: 42

# data

## extract
dataset_name: "rootsautomation/ScreenSpot" # ~1300 samples
data_split: test
data_dir: "screenspot"

## transform
parent_model_path: "OpenGVLab/InternVL2-8B"
max_num: 12
max_new_tokens: 256
do_sample: True
sample_bs: 1

## load
gradient_accumulation_steps: 2 # used to simulate larger batch sizes
batch_size: 4 # if gradient_accumulation_steps > 1, this is the micro-batch size
block_size: 2048 # to fit image + text tokens, TODO: make this dynamic
val_split: 0.1

# Training

## Model
model_path: vikhyatk/moondream2
model_revision: null # https://github.com/vikhyat/moondream/issues/131

## I/O
out_dir: "moondream2-ScreenSpot" # 'out_dir' + str(time.time())
eval_interval: 1
log_interval: 1
eval_iters: 30
eval_only: False # if True, script exits right after the first eval
always_save_checkpoint: True # if True, always save a checkpoint after each eval
init_from: "scratch" # 'scratch' or 'resume'

## wandb logging
wandb_log: False # disabled by default
wandb_project: modeldemo
wandb_run_name: moondream2-ScreenSpot # 'wandb_run_name' + str(time.time())

## adamw optimizer
learning_rate: !!float 1e-5 # max learning rate
max_iters: 300 # total number of training iterations, ~1 epoch
weight_decay: 0.0
beta1: 0.9
beta2: 0.95
eps: !!float 1e-6
grad_clip: 1.0 # clip gradients at this value, or disable if == 0.0

## learning rate decay settings
decay_lr: True # whether to decay the learning rate
warmup_iters: 30 # how many steps to warm up for
lr_decay_iters: 300 # should be ~= max_iters per Chinchilla
min_lr: !!float 1e-6 # minimum learning rate, should be ~= learning_rate/10 per Chinchilla

## DDP settings
backend: "nccl" # 'nccl', 'gloo', etc.

## system
compile: True # use PyTorch 2.0 to compile the model to be faster

## sampling
num_samples: 5
num_beams: 4
no_repeat_ngram_size: 5
early_stopping: True
