import json
import time
import traceback
import requests
from loguru import logger

# Put your own secrets such as API and proxy address in config_private.py
# When reading, first check if there is a private config_private configuration fileï¼ˆNot controlled by gitï¼‰ï¼ŒIf there isï¼ŒThen overwrite the original config file
from void_terminal.toolbox import (
    get_conf,
    update_ui,
    is_the_upload_folder,
)

proxies, TIMEOUT_SECONDS, MAX_RETRY = get_conf(
    "proxies", "TIMEOUT_SECONDS", "MAX_RETRY"
)

timeout_bot_msg = (
    "[Local Message] Request timeout. Network error. Please check proxy settings in config.py."
    + "Network errorï¼ŒCheck if the proxy server is availableï¼ŒAnd if the format of the proxy settings is correctï¼ŒThe format must be[Protocol]://[Address]:[Port]ï¼ŒAll parts are necessaryã€‚"
)


def get_full_error(chunk, stream_response):
    """
    Attempt to get the complete error message
    """
    while True:
        try:
            chunk += next(stream_response)
        except:
            break
    return chunk


def decode_chunk(chunk):
    """
    Used forè§£è¯»"content"and"finish_reason"çš„å†…å®¹
    """
    chunk = chunk.decode()
    respose = ""
    finish_reason = "False"
    try:
        chunk = json.loads(chunk[6:])
    except:
        respose = ""
        finish_reason = chunk
    # Error handling section
    if "error" in chunk:
        respose = "API_ERROR"
        try:
            chunk = json.loads(chunk)
            finish_reason = chunk["error"]["code"]
        except:
            finish_reason = "API_ERROR"
        return respose, finish_reason

    try:
        respose = chunk["choices"][0]["delta"]["content"]
    except:
        pass
    try:
        finish_reason = chunk["choices"][0]["finish_reason"]
    except:
        pass
    return respose, finish_reason


def generate_message(input, model, key, history, max_output_token, system_prompt, temperature):
    """
    Integrate all informationï¼ŒSelect LLM modelï¼ŒGenerate http requestï¼ŒPrepare to send request
    """
    api_key = f"Bearer {key}"

    headers = {"Content-Type": "application/json", "Authorization": api_key}

    conversation_cnt = len(history) // 2

    messages = [{"role": "system", "content": system_prompt}]
    if conversation_cnt:
        for index in range(0, 2 * conversation_cnt, 2):
            what_i_have_asked = {}
            what_i_have_asked["role"] = "user"
            what_i_have_asked["content"] = history[index]
            what_gpt_answer = {}
            what_gpt_answer["role"] = "assistant"
            what_gpt_answer["content"] = history[index + 1]
            if what_i_have_asked["content"] != "":
                if what_gpt_answer["content"] == "":
                    continue
                if what_gpt_answer["content"] == timeout_bot_msg:
                    continue
                messages.append(what_i_have_asked)
                messages.append(what_gpt_answer)
            else:
                messages[-1]["content"] = what_gpt_answer["content"]
    what_i_ask_now = {}
    what_i_ask_now["role"] = "user"
    what_i_ask_now["content"] = input
    messages.append(what_i_ask_now)
    playload = {
        "model": model,
        "messages": messages,
        "temperature": temperature,
        "stream": True,
        "max_tokens": max_output_token,
    }

    return headers, playload


def get_predict_function(
        api_key_conf_name,
        max_output_token,
        disable_proxy = False
    ):
    """
    Generate response function for OpenAI format APIï¼ŒAmong the incoming parametersï¼š
    api_key_conf_nameï¼š
        `config.py`The name of the APIKEY for this modelï¼ŒFor example"YIMODEL_API_KEY"
    max_output_tokenï¼š
        Maximum token count for each requestï¼ŒFor example, for 010,000 items yi-34b-chat-200kï¼ŒIts maximum request number is 4096
        âš ï¸ Do not confuse with the maximum token number of the modelã€‚
    disable_proxyï¼š
        Whether to use a proxyï¼ŒTrue means not in useï¼ŒFalse for useã€‚
    """

    APIKEY = get_conf(api_key_conf_name)

    def predict_no_ui_long_connection(
        inputs,
        llm_kwargs,
        history=[],
        sys_prompt="",
        observe_window=None,
        console_slience=False,
    ):
        """
        Send to chatGPTï¼ŒWaiting for replyï¼ŒCompleted in one goï¼ŒDo not display intermediate processesã€‚But internally use the stream method to avoid the network being cut off midwayã€‚
        inputsï¼š
            This is the input of this inquiry
        sys_prompt:
            System silent prompt
        llm_kwargsï¼š
            Internal tuning parameters of chatGPT
        historyï¼š
            history is the list of previous conversations
        observe_window = Noneï¼š
            Used to transfer the already output part across threadsï¼ŒMost of the time it`s just for fancy visual effectsï¼ŒLeave it blankã€‚observe_window[0]ï¼šObservation windowã€‚observe_window[1]ï¼šWatchdog
        """
        watch_dog_patience = 5  # The patience of the watchdogï¼ŒDisallow biting for 5 seconds(Not biting humans either
        if len(APIKEY) == 0:
            raise RuntimeError(f"APIKEY is empty,Please check the configuration file{APIKEY}")
        if inputs == "":
            inputs = "HelloðŸ‘‹"
        headers, playload = generate_message(
            input=inputs,
            model=llm_kwargs["llm_model"],
            key=APIKEY,
            history=history,
            max_output_token=max_output_token,
            system_prompt=sys_prompt,
            temperature=llm_kwargs["temperature"],
        )
        retry = 0
        while True:
            try:
                from void_terminal.request_llms.bridge_all import model_info

                endpoint = model_info[llm_kwargs["llm_model"]]["endpoint"]
                if not disable_proxy:
                    response = requests.post(
                        endpoint,
                        headers=headers,
                        proxies=proxies,
                        json=playload,
                        stream=True,
                        timeout=TIMEOUT_SECONDS,
                    )
                else:
                    response = requests.post(
                        endpoint,
                        headers=headers,
                        json=playload,
                        stream=True,
                        timeout=TIMEOUT_SECONDS,
                    )
                break
            except:
                retry += 1
                traceback.print_exc()
                if retry > MAX_RETRY:
                    raise TimeoutError
                if MAX_RETRY != 0:
                    logger.error(f"Request timed outï¼ŒRetrying ({retry}/{MAX_RETRY}) â€¦â€¦")

        stream_response = response.iter_lines()
        result = ""
        finish_reason = ""
        while True:
            try:
                chunk = next(stream_response)
            except StopIteration:
                if result == "":
                    raise RuntimeError(f"Get an empty replyï¼ŒPossible reasons:{finish_reason}")
                break
            except requests.exceptions.ConnectionError:
                chunk = next(stream_response)  # Failedï¼ŒRetry onceï¼ŸIf it fails again, there is no wayã€‚
            response_text, finish_reason = decode_chunk(chunk)
            # The returned data stream is empty for the first timeï¼ŒContinue waiting
            if response_text == "" and finish_reason != "False":
                continue
            if response_text == "API_ERROR" and (
                finish_reason != "False" or finish_reason != "stop"
            ):
                chunk = get_full_error(chunk, stream_response)
                chunk_decoded = chunk.decode()
                logger.error(chunk_decoded)
                raise RuntimeError(
                    f"API exception,Please check the terminal outputã€‚Possible reasons are:{finish_reason}"
                )
            if chunk:
                try:
                    if finish_reason == "stop":
                        if not console_slience:
                            print(f"[response] {result}")
                        break
                    result += response_text
                    if observe_window is not None:
                        # Observation windowï¼ŒDisplay the data already obtained
                        if len(observe_window) >= 1:
                            observe_window[0] += response_text
                        # Watchdogï¼ŒIf the dog is not fed beyond the deadlineï¼Œthen terminate
                        if len(observe_window) >= 2:
                            if (time.time() - observe_window[1]) > watch_dog_patience:
                                raise RuntimeError("User canceled the programã€‚")
                except Exception as e:
                    chunk = get_full_error(chunk, stream_response)
                    chunk_decoded = chunk.decode()
                    error_msg = chunk_decoded
                    logger.error(error_msg)
                    raise RuntimeError("Json parsing is not normal")
        return result

    def predict(
        inputs,
        llm_kwargs,
        plugin_kwargs,
        chatbot,
        history=[],
        system_prompt="",
        stream=True,
        additional_fn=None,
    ):
        """
        Send to chatGPTï¼ŒGet output in a streaming wayã€‚
        Used for basic conversation functionsã€‚
        inputs are the inputs for this inquiry
        top_p, Temperature is an internal tuning parameter of chatGPT
        history is the list of previous conversationsï¼ˆNote that both inputs and historyï¼ŒAn error of token overflow will be triggered if the content is too longï¼‰
        chatbot is the conversation list displayed in WebUIï¼ŒModify itï¼ŒThen yield it outï¼ŒYou can directly modify the conversation interface content
        additional_fn represents which button is clickedï¼ŒSee functional.py for buttons
        """
        if len(APIKEY) == 0:
            raise RuntimeError(f"APIKEY is empty,Please check the configuration file{APIKEY}")
        if inputs == "":
            inputs = "HelloðŸ‘‹"
        if additional_fn is not None:
            from void_terminal.core_functional import handle_core_functionality

            inputs, history = handle_core_functionality(
                additional_fn, inputs, history, chatbot
            )
        logger.info(f"[raw_input] {inputs}")
        chatbot.append((inputs, ""))
        yield from update_ui(
            chatbot=chatbot, history=history, msg="Waiting for response"
        )  # Refresh the page

        # check mis-behavior
        if is_the_upload_folder(inputs):
            chatbot[-1] = (
                inputs,
                f"[Local Message] Operation error detected! After you upload the documentï¼ŒClick the `**Function Plugin Area**` button for processingï¼ŒDo not click the `Submit` button or the `Basic Function Area` buttonã€‚",
            )
            yield from update_ui(
                chatbot=chatbot, history=history, msg="Normal"
            )  # Refresh the page
            time.sleep(2)

        headers, playload = generate_message(
            input=inputs,
            model=llm_kwargs["llm_model"],
            key=APIKEY,
            history=history,
            max_output_token=max_output_token,
            system_prompt=system_prompt,
            temperature=llm_kwargs["temperature"],
        )

        history.append(inputs)
        history.append("")
        retry = 0
        while True:
            try:
                from void_terminal.request_llms.bridge_all import model_info

                endpoint = model_info[llm_kwargs["llm_model"]]["endpoint"]
                if not disable_proxy:
                    response = requests.post(
                        endpoint,
                        headers=headers,
                        proxies=proxies,
                        json=playload,
                        stream=True,
                        timeout=TIMEOUT_SECONDS,
                    )
                else:
                    response = requests.post(
                        endpoint,
                        headers=headers,
                        json=playload,
                        stream=True,
                        timeout=TIMEOUT_SECONDS,
                    )
                break
            except:
                retry += 1
                chatbot[-1] = (chatbot[-1][0], timeout_bot_msg)
                retry_msg = (
                    f"ï¼ŒRetrying ({retry}/{MAX_RETRY}) â€¦â€¦" if MAX_RETRY > 0 else ""
                )
                yield from update_ui(
                    chatbot=chatbot, history=history, msg="Request timed out" + retry_msg
                )  # Refresh the page
                if retry > MAX_RETRY:
                    raise TimeoutError

        gpt_replying_buffer = ""

        stream_response = response.iter_lines()
        while True:
            try:
                chunk = next(stream_response)
            except StopIteration:
                break
            except requests.exceptions.ConnectionError:
                chunk = next(stream_response)  # Failedï¼ŒRetry onceï¼ŸIf it fails again, there is no wayã€‚
            response_text, finish_reason = decode_chunk(chunk)
            # The returned data stream is empty for the first timeï¼ŒContinue waiting
            if response_text == "" and finish_reason != "False":
                status_text = f"finish_reason: {finish_reason}"
                yield from update_ui(
                    chatbot=chatbot, history=history, msg=status_text
                )
                continue
            if chunk:
                try:
                    if response_text == "API_ERROR" and (
                        finish_reason != "False" or finish_reason != "stop"
                    ):
                        chunk = get_full_error(chunk, stream_response)
                        chunk_decoded = chunk.decode()
                        chatbot[-1] = (
                            chatbot[-1][0],
                            "[Local Message] {finish_reason},Get the following error messageï¼š\n"
                            + chunk_decoded,
                        )
                        yield from update_ui(
                            chatbot=chatbot,
                            history=history,
                            msg="API exception:" + chunk_decoded,
                        )  # Refresh the page
                        logger.error(chunk_decoded)
                        return

                    if finish_reason == "stop":
                        logger.info(f"[response] {gpt_replying_buffer}")
                        break
                    status_text = f"finish_reason: {finish_reason}"
                    gpt_replying_buffer += response_text
                    # If an exception is thrown hereï¼ŒIt is usually because the text is too longï¼ŒSee the output of get_full_error for details
                    history[-1] = gpt_replying_buffer
                    chatbot[-1] = (history[-2], history[-1])
                    yield from update_ui(
                        chatbot=chatbot, history=history, msg=status_text
                    )  # Refresh the page
                except Exception as e:
                    yield from update_ui(
                        chatbot=chatbot, history=history, msg="Json parsing is not normal"
                    )  # Refresh the page
                    chunk = get_full_error(chunk, stream_response)
                    chunk_decoded = chunk.decode()
                    chatbot[-1] = (
                        chatbot[-1][0],
                        "[Local Message] Parsing error,Get the following error messageï¼š\n" + chunk_decoded,
                    )
                    yield from update_ui(
                        chatbot=chatbot, history=history, msg="Json exception" + chunk_decoded
                    )  # Refresh the page
                    logger.error(chunk_decoded)
                    return

    return predict_no_ui_long_connection, predict
